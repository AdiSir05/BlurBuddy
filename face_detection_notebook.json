{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection Model for Video Privacy System\n",
    "\n",
    "This notebook implements a machine learning model for detecting faces in videos as part of an adaptive face-blur privacy system.\n",
    "\n",
    "## Overview\n",
    "- Uses face images and ground truth bounding boxes/landmarks from YouTube dataset\n",
    "- Trains a neural network to detect faces and predict bounding boxes + landmarks\n",
    "- Provides inference capabilities for face detection in video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install pandas scikit-learn matplotlib pillow opencv-python numpy\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import our custom face detection model\n",
    "from face_detection_model import *\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore dataset from .npz files\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define data directories\n",
    "data_base_dir = \"/Users/adityasirohi/BlurBuddy/Data\"\n",
    "subdirs = [\n",
    "    \"youtube_faces_with_keypoints_full_1/youtube_faces_with_keypoints_full_1\",\n",
    "    \"youtube_faces_with_keypoints_full_2/youtube_faces_with_keypoints_full_2\", \n",
    "    \"youtube_faces_with_keypoints_full_3/youtube_faces_with_keypoints_full_3\",\n",
    "    \"youtube_faces_with_keypoints_full_4/youtube_faces_with_keypoints_full_4\"\n",
    "]\n",
    "\n",
    "# Find all .npz files\n",
    "npz_files = []\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(data_base_dir, subdir)\n",
    "    files = glob.glob(os.path.join(subdir_path, \"*.npz\"))\n",
    "    npz_files.extend(files)\n",
    "\n",
    "print(f\"Found {len(npz_files)} .npz files\")\n",
    "\n",
    "# Extract person names from filenames\n",
    "person_names = []\n",
    "for file_path in npz_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    # Extract person name (everything before the last underscore and number)\n",
    "    person_name = '_'.join(filename.split('_')[:-1])\n",
    "    person_names.append(person_name)\n",
    "\n",
    "# Create a simple DataFrame-like structure\n",
    "data_info = pd.DataFrame({\n",
    "    'file_path': npz_files,\n",
    "    'person_name': person_names\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {data_info.shape}\")\n",
    "print(f\"Unique persons: {data_info['person_name'].nunique()}\")\n",
    "\n",
    "# Show top persons by sample count\n",
    "person_counts = data_info['person_name'].value_counts()\n",
    "print(f\"\\nTop 10 persons by sample count:\")\n",
    "print(person_counts.head(10))\n",
    "\n",
    "# Show data distribution\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"- Total samples: {len(data_info)}\")\n",
    "print(f\"- Unique persons: {data_info['person_name'].nunique()}\")\n",
    "print(f\"- Avg samples per person: {len(data_info) / data_info['person_name'].nunique():.1f}\")\n",
    "\n",
    "# Store for later use\n",
    "df = data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create and Test Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for face detection (we don't need person-based stratification)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "# Create datasets\n",
    "data_dir = \"/Users/adityasirohi/BlurBuddy/Data\"\n",
    "train_dataset = FaceDetectionDataset(train_df, data_dir)\n",
    "val_dataset = FaceDetectionDataset(val_df, data_dir)\n",
    "test_dataset = FaceDetectionDataset(test_df, data_dir)\n",
    "\n",
    "# Test loading a sample\n",
    "sample_image, sample_bbox, sample_landmarks = train_dataset[0]\n",
    "print(f\"\\nSample data shapes:\")\n",
    "print(f\"- Image: {sample_image.shape}\")\n",
    "print(f\"- Bounding box: {sample_bbox.shape}\")\n",
    "print(f\"- Landmarks: {sample_landmarks.shape}\")\n",
    "print(f\"- Bbox values: {sample_bbox}\")\n",
    "print(f\"- First few landmarks: {sample_landmarks[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detection model\n",
    "model = FaceDetectionModel(num_landmarks=68).to(device)\n",
    "\n",
    "print(f\"Model created successfully\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Test model with sample data\n",
    "test_image = torch.zeros((1, 3, 224, 224)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    bbox_output, landmarks_output = model(test_image)\n",
    "    print(f\"Bbox output shape: {bbox_output.shape}\")\n",
    "    print(f\"Landmarks output shape: {landmarks_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample face detection data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, len(train_dataset))):\n",
    "    image, bbox, landmarks = train_dataset[i]\n",
    "    \n",
    "    # Convert image for display\n",
    "    image_display = image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Denormalize coordinates\n",
    "    h, w = image_display.shape[:2]\n",
    "    bbox_denorm = bbox.cpu().numpy() * [w, h, w, h]\n",
    "    landmarks_denorm = landmarks.cpu().numpy().reshape(68, 2) * [w, h]\n",
    "    \n",
    "    # Plot image\n",
    "    axes[i].imshow(image_display)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (bbox_denorm[0], bbox_denorm[1]), \n",
    "        bbox_denorm[2] - bbox_denorm[0], \n",
    "        bbox_denorm[3] - bbox_denorm[1],\n",
    "        linewidth=2, edgecolor='red', facecolor='none'\n",
    "    )\n",
    "    axes[i].add_patch(rect)\n",
    "    \n",
    "    # Draw landmarks\n",
    "    axes[i].scatter(landmarks_denorm[:, 0], landmarks_denorm[:, 1], \n",
    "                   c='blue', s=1, alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(f'Sample {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"\\nShown {min(8, len(train_dataset))} sample faces with bounding boxes and landmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training setup\n",
    "criterion_bbox = nn.MSELoss()  # Mean Squared Error for bounding box regression\n",
    "criterion_landmarks = nn.MSELoss()  # Mean Squared Error for landmarks regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5, factor=0.5)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Smaller batch size for face detection\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Training in {len(train_loader)} batches of {batch_size} samples each\")\n",
    "\n",
    "# Training variables\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bbox_losses = []\n",
    "landmarks_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"Note: This may take several minutes depending on your hardware\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Reduced epochs for demonstration\n",
    "    print(f\"\\nEpoch {epoch + 1}/20:\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_bbox_loss, train_landmarks_loss = train_epoch(\n",
    "        model, train_loader, criterion_bbox, criterion_landmarks, optimizer, device\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    bbox_losses.append(train_bbox_loss)\n",
    "    landmarks_losses.append(train_landmarks_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_bbox_loss, val_landmarks_loss = evaluate(\n",
    "        model, val_loader, criterion_bbox, criterion_landmarks, device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} (Bbox: {train_bbox_loss:.4f}, Landmarks: {train_landmarks_loss:.4f})\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} (Bbox: {val_bbox_loss:.4f}, Landmarks: {val_landmarks_loss:.4f})\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'epoch': epoch,\n",
    "        }, 'face_detection_model.pth')\n",
    "        no_improve = 0\n",
    "        print(f\"  -> Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    \n",
    "    # Early stopping check\n",
    "    if no_improve >= patience:\n",
    "        print(f\"  -> Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if torch.cuda.is_available():\n",
    "    checkpoint = torch.load('face_detection_model.pth')\n",
    "else:\n",
    "    checkpoint = torch.load('face_detection_model.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Visualize predictions on test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "visualize_predictions(model, test_loader, device, num_samples=4)\n",
    "\n",
    "print(\"Model predictions visualized above!\")\n",
    "print(\"Red boxes and dots = Model predictions\")\n",
    "print(\"Green boxes and dots = Ground truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# Plot total losses\n",
    "ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(epochs, val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Total Loss')\n",
    "ax1.set_title('Training Progress - Total Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot bbox losses\n",
    "ax2.plot(epochs, bbox_losses, label='Bbox Loss', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Bbox Loss')\n",
    "ax2.set_title('Bounding Box Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot landmarks losses\n",
    "ax3.plot(epochs, landmarks_losses, label='Landmarks Loss', color='orange')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Landmarks Loss')\n",
    "ax3.set_title('Landmarks Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# Plot combined losses\n",
    "ax4.plot(epochs, train_losses, label='Train Total', color='blue')\n",
    "ax4.plot(epochs, bbox_losses, label='Train Bbox', color='green')\n",
    "ax4.plot(epochs, landmarks_losses, label='Train Landmarks', color='orange')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Loss')\n",
    "ax4.set_title('All Training Losses')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training Summary:\")\n",
    "print(f\"- Total epochs: {len(train_losses)}\")\n",
    "print(f\"- Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"- Final val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"- Best val loss: {min(val_losses):.4f}\")\n",
    "print(f\"- Model saved to: face_detection_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage for Real-Time Video Processing\n",
    "\n",
    "The trained model can now be used to detect faces in video streams:\n",
    "\n",
    "```python\n",
    "# Example for video frame face detection:\n",
    "bbox, landmarks = detect_face(model, frame_image, device)\n",
    "```\n",
    "\n",
    "This completes the face detection part of your adaptive face-blur privacy system. The next steps would be:\n",
    "\n",
    "1. **Temporal Tracking**: Use optical flow to track faces across frames\n",
    "2. **Avatar Generation**: Create privacy-preserving avatars for each detected face\n",
    "3. **Face Replacement**: Replace detected faces with consistent pseudonymous avatars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
